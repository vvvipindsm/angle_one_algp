{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging: Implement a bagging model\n",
    "\n",
    "Using the Titanic dataset from [this](https://www.kaggle.com/c/titanic/overview) Kaggle competition.\n",
    "\n",
    "In this section, we will fit and evaluate a simple Random Forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data\n",
    "\n",
    "_In this final video in the bagging chapter we're going to try to build the best Random Forest model we can on this Titanic dataset by using the same process we did for Gradient Boosting - we will search for the best hyperparameter settings for the Random Forest model using `GridSearchCV`._\n",
    "\n",
    "_Lets start by importing the same packages we imported last chapter - so that is `joblib` to save out our model, `pandas` to read in our data, and then our classifier and `GridSearchCV` for `sklearn`. And then we will read in the training features and the training labels._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tr_features = pd.read_csv('../train_features.csv')\n",
    "tr_labels = pd.read_csv('../train_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "_Recall this helper function we used in the last chapter to help us print out the average accuracy score and the standard deviation of that accuracy score (across the 5 folds built into our Cross Validation) for each hyperparameter combination._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results):\n",
    "    print('BEST PARAMS: {}\\n'.format(results.best_params_))\n",
    "\n",
    "    means = results.cv_results_['mean_test_score']\n",
    "    stds = results.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n",
    "        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_So lets walk through this code again, this should look familiar as it's basically the same as what we ran for GradientBoosting. We have the `RandomForestClassifier` object and we've stored it as `rf`.\n",
    "\n",
    "_Then we define our hyperparameter dectionary. One more reminder that the keys in this dictionary align with the name of the hyperparameters that would be passed into `RandomForestClassifier`_\n",
    "\n",
    "_And then we just need to define the list of settings we want to test for each hyperparameter. So for number of estimators we will test out building 5 decision trees, 50 decision trees, 250 decision trees, and 500 decision trees._\n",
    "\n",
    "_And then for max depth we test 4, 8, 16, 32, and None. None will just build it as deep as it wants until it achieves some level of training error tolerance defined within `RandomForestClassifier`._\n",
    "\n",
    "_Again, we are testing deeper trees here than we did for Gradient Boosting and that's expected based on the way we know these two algorithms optimize that bias/variance tradeoff. Random Forest starts with deep trees that have high variance and low bias._\n",
    "\n",
    "_So now call `GridSearchCV`, pass in our model object (`rf`), the hyperparameter dictionary, and tell it we want to do 5-fold Cross Validation._\n",
    "\n",
    "_We just call `.fit()` with our training features and labels (and convert the column vector to an array). And then `GridSearchCV` will run 5-fold Cross Validation for each hyperparameter setting combination._\n",
    "\n",
    "_And then we will just print out our results using the `print results()` function._\n",
    "\n",
    "_So lets go ahead and run this._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST PARAMS: {'max_depth': 8, 'n_estimators': 250}\n",
      "\n",
      "0.809 (+/-0.098) for {'max_depth': 4, 'n_estimators': 5}\n",
      "0.813 (+/-0.106) for {'max_depth': 4, 'n_estimators': 50}\n",
      "0.824 (+/-0.108) for {'max_depth': 4, 'n_estimators': 250}\n",
      "0.826 (+/-0.106) for {'max_depth': 4, 'n_estimators': 500}\n",
      "0.813 (+/-0.073) for {'max_depth': 8, 'n_estimators': 5}\n",
      "0.818 (+/-0.076) for {'max_depth': 8, 'n_estimators': 50}\n",
      "0.828 (+/-0.067) for {'max_depth': 8, 'n_estimators': 250}\n",
      "0.818 (+/-0.07) for {'max_depth': 8, 'n_estimators': 500}\n",
      "0.792 (+/-0.029) for {'max_depth': 16, 'n_estimators': 5}\n",
      "0.811 (+/-0.029) for {'max_depth': 16, 'n_estimators': 50}\n",
      "0.811 (+/-0.029) for {'max_depth': 16, 'n_estimators': 250}\n",
      "0.805 (+/-0.021) for {'max_depth': 16, 'n_estimators': 500}\n",
      "0.79 (+/-0.039) for {'max_depth': 32, 'n_estimators': 5}\n",
      "0.803 (+/-0.036) for {'max_depth': 32, 'n_estimators': 50}\n",
      "0.807 (+/-0.032) for {'max_depth': 32, 'n_estimators': 250}\n",
      "0.817 (+/-0.03) for {'max_depth': 32, 'n_estimators': 500}\n",
      "0.805 (+/-0.055) for {'max_depth': None, 'n_estimators': 5}\n",
      "0.805 (+/-0.067) for {'max_depth': None, 'n_estimators': 50}\n",
      "0.807 (+/-0.025) for {'max_depth': None, 'n_estimators': 250}\n",
      "0.811 (+/-0.034) for {'max_depth': None, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "parameters = {\n",
    "    'n_estimators': [5, 50, 250, 500],\n",
    "    'max_depth': [4, 8, 16, 32, None]\n",
    "}\n",
    "cv = GridSearchCV(rf, parameters, cv=5)\n",
    "cv.fit(tr_features, tr_labels.values.ravel())\n",
    "\n",
    "print_results(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Before we dig into the results, I want to call out that with `RandomForest`, even if I ran this exact cell again on the SAME exact training set - I would get different results. That's because each time you run `RandomForest` it is randomly sampling rows and columns internally (like we discussed earlier in this chapter) to build each decision tree. So you'll get different results each time you run `RandomForest`._\n",
    "\n",
    "_Also - remember that these results are on unseen data thanks to the way the Cross Validation built into GridSearchCV splits up the data._\n",
    "\n",
    "_Now, looking at the results. The best results are using 50 estimators with a max depth of 4, that generates an accuracy of 82.8% so this is the best Cross-Validation performance that we've seen thus far. Feel free to dig through these results but there are two things I want to call out quickly:_\n",
    "1. _It's clear that 5 estimators isn't quite enough as for every combination, the ones with only 5 estimators generate the worst results._\n",
    "2. _As you scroll through the results you can see all parameter combinations with max_depth of 4 do quite well, they also do quite well with max depth of 8 but it starts to fall off after that which indicates that for some of these deeper trees, they might be overfitting just a little bit._\n",
    "\n",
    "_Then lets ask the `GridSearchCV` object to print out the best model based on performance on unseen data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=8, n_estimators=250)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out pickled model\n",
    "\n",
    "_Lastly, we'll just write out the fit model using `joblib.dump()` - we just pass in the model object and just tell it to write to the same location where we stored our Gradient Boosting model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/RF_model.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(cv.best_estimator_, '../models/RF_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In the next chapter, we will cover the last of our ensemble techniques...stacking._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
