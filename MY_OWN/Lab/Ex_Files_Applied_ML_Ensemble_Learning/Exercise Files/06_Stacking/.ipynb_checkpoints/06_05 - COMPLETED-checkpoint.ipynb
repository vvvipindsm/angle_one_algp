{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking: Explore stacking algorithms in Python\n",
    "\n",
    "Import [`StackingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html) from `sklearn` and explore the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Stacking Algorithm for Classification\n",
    "\n",
    "_Just as we did for boosting and bagging previously, in this video we will explore some of the key hyperparameters for the `StackingClassifier` in `sklearn`! Again, we are only looking at the CLASSIFIER here since our titanic dataset is a classification problem but there is also a `StackingRegressor` tool in `sklearn`._\n",
    "\n",
    "_Lets start by importing `StackingClassifier` from `sklearn.ensemble` and here you will see one difference between this algorithm and the others we have looked at previously. Lets try to call the parameters in the same way we did with the prior models, by calling the `get params` method._\n",
    "\n",
    "_You'll see we get an error that basically says it expects an `estimators` argument to be passed into `StackingClassifier`. The gradient boosting and random forest classifiers had no such requirements. This `estimators` argument is a list of algorithms to use for the base estimators, you can check out the docs linked above if you want to read more about the argument. So lets pass in a list containing the two algorithms we have already looked at._\n",
    "\n",
    "_So first we need to import gradient boosting classifier and random forest classifier. Then we are going to define our list of estimators and this is actually going to be a list of tuples. The first entry in the tuple will be what you want to name the model and the second entry will be the algorithm. So lets first do gradient boosting which we will name `gb` and we will leave the parentheses empty so it uses the default values for now. Then lets do random forest which we will name `rf` and again we will leave paretheses empty so it uses default values. Then lets add the estimators argument to Stacking Classifier and try calling get params again._\n",
    "\n",
    "_So here are all of the potential hyperparameters we could tune. This looks like a long glance at first but lets start at the bottom and work our way up._\n",
    "\n",
    "_You'll notice all of these arguments with the `rf` prefix are all of the hyperparameters for the random forest model. So we aren't only tuning the hyperparameters for the stacking algorithm but we are also tuning the hyperparameters for the base models (random forest in this case). If you think about it, we did the same exact thing with gradient boosting and random forest. When we set the max depth, that was setting a hyperparameter for the base model. It's just that now with stacking, we are building on base models that have a lot more hyperparameters._\n",
    "\n",
    "_So we see all of these hyperparameters for random forest and then right above it you'll see a bunch of hyperparameters with the `gb` prefix and those are all of the hyperparameters associated with gradient boosting. Then we start to get into the actual stacking hyperparameters and we are going to focus on three parameters here._\n",
    "1. _`estimators` - we already briefly covered this but it's a list of tuples that define the base models you want to use, you can do multiple models of the same type with different hyperparameters. So we could have one RandomForest model with 50 trees and another with 150, if we wanted._\n",
    "2. _`final_estimator` - this is the meta-model that is trained on the output of all of the base models._\n",
    "3. _`passthrough` - this parameter allows you to pass the original data into the meta-model or the final estimator. If it's set to false, it will only train on the predictions from the base models. If it's true, then it will pass the original training data into the final estimator in addition to the predictions from the base models_\n",
    "\n",
    "_In the next lesson we will fit a stacked model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv': None,\n",
       " 'estimators': [('gb', GradientBoostingClassifier()),\n",
       "  ('rf', RandomForestClassifier())],\n",
       " 'final_estimator': None,\n",
       " 'n_jobs': None,\n",
       " 'passthrough': False,\n",
       " 'stack_method': 'auto',\n",
       " 'verbose': 0,\n",
       " 'gb': GradientBoostingClassifier(),\n",
       " 'rf': RandomForestClassifier(),\n",
       " 'gb__ccp_alpha': 0.0,\n",
       " 'gb__criterion': 'friedman_mse',\n",
       " 'gb__init': None,\n",
       " 'gb__learning_rate': 0.1,\n",
       " 'gb__loss': 'deviance',\n",
       " 'gb__max_depth': 3,\n",
       " 'gb__max_features': None,\n",
       " 'gb__max_leaf_nodes': None,\n",
       " 'gb__min_impurity_decrease': 0.0,\n",
       " 'gb__min_impurity_split': None,\n",
       " 'gb__min_samples_leaf': 1,\n",
       " 'gb__min_samples_split': 2,\n",
       " 'gb__min_weight_fraction_leaf': 0.0,\n",
       " 'gb__n_estimators': 100,\n",
       " 'gb__n_iter_no_change': None,\n",
       " 'gb__presort': 'deprecated',\n",
       " 'gb__random_state': None,\n",
       " 'gb__subsample': 1.0,\n",
       " 'gb__tol': 0.0001,\n",
       " 'gb__validation_fraction': 0.1,\n",
       " 'gb__verbose': 0,\n",
       " 'gb__warm_start': False,\n",
       " 'rf__bootstrap': True,\n",
       " 'rf__ccp_alpha': 0.0,\n",
       " 'rf__class_weight': None,\n",
       " 'rf__criterion': 'gini',\n",
       " 'rf__max_depth': None,\n",
       " 'rf__max_features': 'auto',\n",
       " 'rf__max_leaf_nodes': None,\n",
       " 'rf__max_samples': None,\n",
       " 'rf__min_impurity_decrease': 0.0,\n",
       " 'rf__min_impurity_split': None,\n",
       " 'rf__min_samples_leaf': 1,\n",
       " 'rf__min_samples_split': 2,\n",
       " 'rf__min_weight_fraction_leaf': 0.0,\n",
       " 'rf__n_estimators': 100,\n",
       " 'rf__n_jobs': None,\n",
       " 'rf__oob_score': False,\n",
       " 'rf__random_state': None,\n",
       " 'rf__verbose': 0,\n",
       " 'rf__warm_start': False}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "estimators = [('gb', GradientBoostingClassifier()), ('rf', RandomForestClassifier())]\n",
    "StackingClassifier(estimators = estimators).get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
