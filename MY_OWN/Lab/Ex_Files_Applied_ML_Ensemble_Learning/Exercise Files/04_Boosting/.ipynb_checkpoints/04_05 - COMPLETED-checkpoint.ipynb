{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting: Explore boosting algorithms in Python\n",
    "\n",
    "Import [`GradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) and [`AdaBoostClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) from `sklearn` and explore the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Boosting Algorithm for Classification\n",
    "\n",
    "_In this video we're going to import the gradient boosting and adaptive boosting classifiers used in Python and explore some of the key hyperparameters to tune. We are only looking at CLASSIFIERS here since our titanic dataset is a classification problem but there are also equivalent tools in Python for regression._\n",
    "\n",
    "_Lets start by importing both `GradientBoostingClassifier` and `AdaBoostClassifier` from `sklearn.ensemble`._\n",
    "\n",
    "_We are going to start by exploring the hyperparameters for Gradient Boosting. We can view all available hyperparameters by calling the `get params` method. There are a lot of hyperparameters we could explore here and I encourage you to explore those in the documentation linked above. In the interest of time, I am just going to focus on some of the most important hyperparameters._\n",
    "\n",
    "_Remember, Gradient Boosting is made up of a lot of really shallow decision trees. So `n estimators` controls the number of trees the algorithm should create...and the default is set to 100. And `max depth` is the maximum depth of those trees...and the default is set to 3, which is quite shallow. So by default, Gradient Boosting would build 100 trees of depth 3._\n",
    "\n",
    "_Lastly, there is a `learning rate` parameter that effectively controls how quickly the algorithm attempts to optimize. If `learning rate` is really low, it may take longer for the algorithm to fit and it may not end up finding the optimal model...it may get stuck in what's called a local minimum, meaning it finds a pretty good answer but not the BEST answer (which would be called a global minimum). On the other side, if you set learning rate really high, the model will fit quicker but again, you run the risk of finding a sub-optimal model. So in other words, learning rate allows you to control the balance between the time to fit and how well the model fits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccp_alpha': 0.0,\n",
       " 'criterion': 'friedman_mse',\n",
       " 'init': None,\n",
       " 'learning_rate': 0.1,\n",
       " 'loss': 'deviance',\n",
       " 'max_depth': 3,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_iter_no_change': None,\n",
       " 'presort': 'deprecated',\n",
       " 'random_state': None,\n",
       " 'subsample': 1.0,\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "\n",
    "GradientBoostingClassifier().get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Now lets take a look at the hyperparameters for the adaptive boosting classifier by calling that same `get params` method. And you'll see there is a much smaller set of hyperparameters here._\n",
    "\n",
    "_Starting with algorithm - this is the actual boosting algorithm used and the default is the true boosting algorithm, you'll want to leave it as that. `base estimator` refers to the type of weak models you will use. Remember one of the differences between grdient boosting and adaptive boosting is gradient boosting requires the use of decision trees for the base models but you don't need to use decisino trees in adaptive boosting, you can use whatever algorithm you'd like._\n",
    "\n",
    "_`learning rate` is the same as it is for Gradient Boosting, it controls the tradeoff between time to fit and how well the model fits. And `n estimators` is the number of base or weak models you build, by default it is set to build 50 of whatever model you declare in `base estimator`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'SAMME.R',\n",
       " 'base_estimator': None,\n",
       " 'learning_rate': 1.0,\n",
       " 'n_estimators': 50,\n",
       " 'random_state': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AdaBoostClassifier().get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In the next and final video in this chapter, we are going to actually fit a Gradient boosting model!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
