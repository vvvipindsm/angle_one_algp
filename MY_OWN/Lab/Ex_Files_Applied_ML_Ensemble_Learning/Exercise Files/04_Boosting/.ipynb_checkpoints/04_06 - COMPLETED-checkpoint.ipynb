{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting: Implement a boosting model\n",
    "\n",
    "Using the Titanic dataset from [this](https://www.kaggle.com/c/titanic/overview) Kaggle competition.\n",
    "\n",
    "In this section, we will fit and evaluate a simple Gradient Boosting model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data\n",
    "\n",
    "_In this FINAL video of this chapter, we'll fit the best Gradient Boosting model we can by using `GridSearchCV` to tune three key hyperparameters. If you want to learn more about GridSearchCV, you should take my Applied Machine Learning Fonudations course that talks more about the proper framework to fit and evaluate models. In short, GridSearchCV allows us to easily search through a number of different hyperparameter setting combinations to find the one that generates the best performance on unseen data._\n",
    "\n",
    "_So we will find the best boosted model we can in this video and we will save that fit model. We will do the same in the bagging and stacking sections. Then in the last chapter of this course we will compare the best boosting, bagging, and stacking models against one another on the validation set to see which model performs best._\n",
    "\n",
    "_Lets start by importing a few packages:_\n",
    "* _`joblib` will help us save our fit model at the end of this lesson_\n",
    "* _`pandas` to read our data into a dataframe_\n",
    "* _and then `GradientBoostingClassifier` and `GridSearchCV` from `sklearn`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tr_features = pd.read_csv('../train_features.csv')\n",
    "tr_labels = pd.read_csv('../train_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "_A quick reminder of what the three hyperparameters we will be tuning represent:_\n",
    "- _Number of estimators simply represents how many individual decision trees to build_\n",
    "- _Max depth dictates how deep each of those trees can go_\n",
    "- _Learning rate controls how quickly this algorithm will try to find the optimal model - too large and it will never find the optimal solution, too small and it also may not find the optimal solution and even if it does, it will take a long time to do so_\n",
    "\n",
    "_Now, the `GridSearchCV` method stores a LOT of information about model performance but it can be kind of difficult to pick through to find what you need. So I wrote a quick little function here for us to use to print the results a little more cleanly. I'm not going to go through it in detail but in essence what it does is for every hyper-parameter combination it will print out the average accuracy score and the standard deviation of that accuracy score (across the 5 folds built into our Cross Validation). This will give us the information we need to select the optimal hyperparameter settings._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results):\n",
    "    print('BEST PARAMS: {}\\n'.format(results.best_params_))\n",
    "\n",
    "    means = results.cv_results_['mean_test_score']\n",
    "    stds = results.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n",
    "        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ok - lets get into the actual process of searching for the best hyperparameter settings using GridSearchCV. So we start by calling the `GradientBoostingClassifier` object and store it as `gb`. If we wanted to hardcode any hyperparameter values in we would enter them in these parentheses. Otherwise, it will just use the defaults which we saw in the last lesson. We don't want to hardcode any values at this moment because we want to use GridSearchCV to test different hyperparameter settings._\n",
    "\n",
    "_The first thing we need to do for `GridSearchCV` is define our hyperparameter dictionary. The hyperparameters we want to tune are `n estimators`, `max depth`, and `learning rate` (all of the rest will be set as their default values)._\n",
    "\n",
    "_So for number of estimators we will test out building 5, 50, 250, and 500 decision trees._\n",
    "\n",
    "_And then for max depth we will start with 1 (what's called a decision stump) and then we'll also test out 3, 5, 7, and 9._\n",
    "\n",
    "_So now call `GridSearchCV`, pass in our model object (`gb`), the hyperparameter dictionary, and tell it we want to do 5-fold Cross Validation._\n",
    "\n",
    "_We just call `.fit()` with our training features and labels and that will fit a model with each hyperparamater combination and evaluate them to see which is the best one. One note here - `tr labels` are stored as a column vector type but what `sklearn` really wants them to be is an array - so we will just convert it from the a column vector to an array using `.values.ravel()`._\n",
    "\n",
    "_Lastly, lets call our print results function to show us how each model performed._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST PARAMS: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500}\n",
      "\n",
      "0.624 (+/-0.007) for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.796 (+/-0.115) for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.796 (+/-0.115) for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.811 (+/-0.117) for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500}\n",
      "0.624 (+/-0.007) for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.811 (+/-0.069) for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.83 (+/-0.074) for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.841 (+/-0.077) for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500}\n",
      "0.624 (+/-0.007) for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.82 (+/-0.051) for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.82 (+/-0.037) for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.82 (+/-0.036) for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500}\n",
      "0.624 (+/-0.007) for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.817 (+/-0.05) for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.817 (+/-0.043) for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.803 (+/-0.031) for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 500}\n",
      "0.624 (+/-0.007) for {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 5}\n",
      "0.803 (+/-0.059) for {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 50}\n",
      "0.798 (+/-0.043) for {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 250}\n",
      "0.788 (+/-0.05) for {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 500}\n",
      "0.796 (+/-0.115) for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.815 (+/-0.119) for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.818 (+/-0.111) for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.828 (+/-0.092) for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500}\n",
      "0.813 (+/-0.071) for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.841 (+/-0.07) for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.818 (+/-0.023) for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.811 (+/-0.036) for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500}\n",
      "0.817 (+/-0.045) for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.824 (+/-0.025) for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.803 (+/-0.035) for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.802 (+/-0.037) for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 500}\n",
      "0.82 (+/-0.051) for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.8 (+/-0.016) for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.79 (+/-0.038) for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.788 (+/-0.03) for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500}\n",
      "0.8 (+/-0.05) for {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 5}\n",
      "0.794 (+/-0.035) for {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 50}\n",
      "0.798 (+/-0.056) for {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250}\n",
      "0.792 (+/-0.036) for {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 500}\n",
      "0.818 (+/-0.099) for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.832 (+/-0.081) for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.826 (+/-0.077) for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.822 (+/-0.081) for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 500}\n",
      "0.824 (+/-0.057) for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.794 (+/-0.034) for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.788 (+/-0.043) for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.79 (+/-0.028) for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 500}\n",
      "0.792 (+/-0.033) for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.792 (+/-0.043) for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.787 (+/-0.045) for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.794 (+/-0.054) for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 500}\n",
      "0.781 (+/-0.038) for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.798 (+/-0.039) for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.794 (+/-0.04) for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.792 (+/-0.061) for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 500}\n",
      "0.77 (+/-0.047) for {'learning_rate': 1, 'max_depth': 9, 'n_estimators': 5}\n",
      "0.786 (+/-0.026) for {'learning_rate': 1, 'max_depth': 9, 'n_estimators': 50}\n",
      "0.809 (+/-0.025) for {'learning_rate': 1, 'max_depth': 9, 'n_estimators': 250}\n",
      "0.8 (+/-0.05) for {'learning_rate': 1, 'max_depth': 9, 'n_estimators': 500}\n",
      "0.204 (+/-0.115) for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.204 (+/-0.115) for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.204 (+/-0.115) for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.204 (+/-0.115) for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 500}\n",
      "0.307 (+/-0.195) for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.307 (+/-0.195) for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.307 (+/-0.195) for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.307 (+/-0.195) for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 500}\n",
      "0.415 (+/-0.258) for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.443 (+/-0.263) for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.414 (+/-0.258) for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.442 (+/-0.257) for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 500}\n",
      "0.575 (+/-0.141) for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.633 (+/-0.169) for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.579 (+/-0.211) for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.597 (+/-0.166) for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 500}\n",
      "0.697 (+/-0.114) for {'learning_rate': 10, 'max_depth': 9, 'n_estimators': 5}\n",
      "0.727 (+/-0.12) for {'learning_rate': 10, 'max_depth': 9, 'n_estimators': 50}\n",
      "0.7 (+/-0.118) for {'learning_rate': 10, 'max_depth': 9, 'n_estimators': 250}\n",
      "0.691 (+/-0.117) for {'learning_rate': 10, 'max_depth': 9, 'n_estimators': 500}\n",
      "0.376 (+/-0.007) for {'learning_rate': 100, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.376 (+/-0.007) for {'learning_rate': 100, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.376 (+/-0.007) for {'learning_rate': 100, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.376 (+/-0.007) for {'learning_rate': 100, 'max_depth': 1, 'n_estimators': 500}\n",
      "0.29 (+/-0.102) for {'learning_rate': 100, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.29 (+/-0.102) for {'learning_rate': 100, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.29 (+/-0.102) for {'learning_rate': 100, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.29 (+/-0.102) for {'learning_rate': 100, 'max_depth': 3, 'n_estimators': 500}\n",
      "0.357 (+/-0.186) for {'learning_rate': 100, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.359 (+/-0.19) for {'learning_rate': 100, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.354 (+/-0.185) for {'learning_rate': 100, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.348 (+/-0.187) for {'learning_rate': 100, 'max_depth': 5, 'n_estimators': 500}\n",
      "0.579 (+/-0.105) for {'learning_rate': 100, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.586 (+/-0.114) for {'learning_rate': 100, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.584 (+/-0.089) for {'learning_rate': 100, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.595 (+/-0.083) for {'learning_rate': 100, 'max_depth': 7, 'n_estimators': 500}\n",
      "0.669 (+/-0.111) for {'learning_rate': 100, 'max_depth': 9, 'n_estimators': 5}\n",
      "0.674 (+/-0.085) for {'learning_rate': 100, 'max_depth': 9, 'n_estimators': 50}\n",
      "0.674 (+/-0.104) for {'learning_rate': 100, 'max_depth': 9, 'n_estimators': 250}\n",
      "0.706 (+/-0.086) for {'learning_rate': 100, 'max_depth': 9, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "parameters = {\n",
    "    'n_estimators': [5, 50, 250, 500],\n",
    "    'max_depth': [1, 3, 5, 7, 9],\n",
    "    'learning_rate': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "cv = GridSearchCV(gb, parameters, cv=5)\n",
    "cv.fit(tr_features, tr_labels.values.ravel())\n",
    "\n",
    "print_results(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_I want to note two things here:_\n",
    "1. _These results are still on unseen data, the Cross Validation built into `GridSearchCV` splits the training data into k parts, trains the model on k-1 parts, and then evaluates the model on the last chunk of data_\n",
    "2. _If you're running this along with me at home, you might have a different training set than I do and there is some randomization built into some of these algorithms. So it's very possible that you may get different results than I do. In fact, I could even run this cell again and get slightly different results._\n",
    "\n",
    "_There are A LOT of hyperparameter combinations here. Remember, we tested 4 levels of `n estimators`, 5 levels of `max depth`, and 5 levels of `learning rate` so that makes for 100 TOTAL MODELS built._\n",
    "\n",
    "_We can see that the best model has a learning rate of 0.01, max depth of 3, and 500 total estimators. That combination is generating an accuracy of 84.1%._\n",
    "\n",
    "_Two things I'll point out in the results:_\n",
    "1. _High learning rate is generating really poor results across the board, indicating that it's jumping across that loss curve too quickly and not finding the optimal model_\n",
    "2. _Models with 5 estimators is pretty consistently the worst model but depending on the other settings, we have 50, 250, and 500 estimators all generating fairly good models._\n",
    "\n",
    "_Feel free to dig through these results yourselves and see what other insights you may be able to pull out that will help build your intuition for future model builds._\n",
    "\n",
    "_Next, lets call the `best estimator` attribute from this `GridSearchCV` object and it will return the fit model tha performed best on unseen data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=0.01, n_estimators=500)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out pickled model\n",
    "\n",
    "_Lastly, lets write out this model to compare it to the other models in the last chapter of this course. We just call `joblib.dump()` and pass in the model object and just tell it to write to our models folder. And it's important to remember that this is saving your model that has been fit on the training data. So once it's saved, we can read it back into Python and start making predictions on data it has never seen before._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/GB_model.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(cv.best_estimator_, '../models/GB_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Hopefully this chapter has given you a pretty good grasp of boosting and how to implement it in Python. In the next chapter, we are going to take a looking at a different type of ensemble learning called bagging._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
