{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZK36hW6M7cl"
   },
   "source": [
    "#Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2554,
     "status": "ok",
     "timestamp": 1621008666318,
     "user": {
      "displayName": "Diogo Resende",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgH4DE06-yE1oiIzCiaAURCl1Escr4LXXiaSnbSaA=s64",
      "userId": "05706604408624562002"
     },
     "user_tz": -120
    },
    "id": "xBzm1qtYNE6M"
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from prophet import Prophet\n",
    "import yfinance\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "from datetime import datetime\n",
    "from ta import add_all_ta_features\n",
    "\n",
    "from dataProcessing import DataProcessing\n",
    "# Remove Future Warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Machine Learning\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Reporting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_setup_data(sybmol):\n",
    "    df = pd.read_csv(\"../stock_historical_data/{}.csv\".format(sybmol))\n",
    "  #  df.set_index(\"Date\", inplace=True)\n",
    "   # new_data = pd.DataFrame(input_data)\n",
    "   # new_data.set_index(\"Date\", inplace=True)\n",
    "    #concatenated_df = pd.concat([df,new_data])\n",
    "    return df\n",
    "\n",
    "def fetch_stock_data(tickers):\n",
    "    data = yfinance.download(tickers,period='1d')\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  2 of 2 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vipin/opt/anaconda3/envs/my_own/lib/python3.9/site-packages/ta/trend.py:780: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  dip[idx] = 100 * (self._dip[idx] / value)\n",
      "/Users/vipin/opt/anaconda3/envs/my_own/lib/python3.9/site-packages/ta/trend.py:785: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  din[idx] = 100 * (self._din[idx] / value)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'dayofweek'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m  data \u001b[38;5;241m=\u001b[39m add_all_ta_features(data, \u001b[38;5;28mopen\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpen\u001b[39m\u001b[38;5;124m\"\u001b[39m, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHigh\u001b[39m\u001b[38;5;124m\"\u001b[39m, low\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLow\u001b[39m\u001b[38;5;124m\"\u001b[39m, close\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m\"\u001b[39m, volume\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVolume\u001b[39m\u001b[38;5;124m\"\u001b[39m, fillna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     34\u001b[0m  data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mindex\n\u001b[0;32m---> 35\u001b[0m  data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweekdays\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdayofweek\u001b[49m\n\u001b[1;32m     36\u001b[0m  \u001b[38;5;28mprint\u001b[39m(data)\n\u001b[1;32m     38\u001b[0m  \u001b[38;5;66;03m# Find NaN Rows\u001b[39;00m\n\u001b[1;32m     39\u001b[0m  \u001b[38;5;66;03m#na_list = data.columns[data.isna().any().tolist()]\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# data.drop(columns=na_list, inplace=True)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m  \u001b[38;5;66;03m# Handle inf values\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/my_own/lib/python3.9/site-packages/pandas/core/generic.py:5989\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5983\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5984\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5985\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5986\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5987\u001b[0m ):\n\u001b[1;32m   5988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'dayofweek'"
     ]
    }
   ],
   "source": [
    "#symbols = [\"CONCOR.NS\",\"ELGIEQUIP.NS\"]\n",
    "#symbols = [\"OLECTRA.NS\",\"CONCOR.NS\",\"ELGIEQUIP.NS\",\"IOC.NS\",\"BEL.NS\",\"TATAELXSI.NS\",\"^NSEI\",\"RELI\",\"HDFCBANK.NS\",\"TATAMOTORS.NS\",\"SBIN.NS\",\"TCS.NS\",\"TITAN.NS\",\"SUNPHARMA.BO\",\"TECHM.NS\", \"ASIANPAINT.NS\",\"TATACONSUM.NS\"]\n",
    "\n",
    "symbols = [\"^NSEI\",\"RELI\"]\n",
    "\n",
    "Icdate = 0\n",
    "Iresult = 1\n",
    "IisClassification = 2\n",
    "Imodel = 3\n",
    "Istock = 4\n",
    "Iclose = 5\n",
    "Iprob = 6\n",
    "  # Add the tickers you want to fetch data for\n",
    "loaded_models = {}\n",
    "result = []\n",
    "\n",
    "final_result = {}\n",
    "final_target = {}\n",
    "stock_data = fetch_stock_data(symbols)\n",
    "#print(stock_data)\n",
    "finaldata = {}\n",
    "current_date = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "\n",
    "for symbol in symbols:\n",
    "    stock_name = symbol\n",
    "    if symbol == \"^NSEI\":\n",
    "        stock_name = \"NSEI\"\n",
    " \n",
    "    data = load_and_setup_data(symbol)\n",
    "    data = data[[\"Open\", \"High\", \"Low\", \"Close\",\"Volume\"]]\n",
    "    data['weekdays'] = 1\n",
    "   # data.to_csv(f\"../stock_historical_data/{symbol}.csv\")\n",
    "    data = add_all_ta_features(data, open=\"Open\", high=\"High\", low=\"Low\", close=\"Close\", volume=\"Volume\", fillna=True)\n",
    "    print(data)\n",
    "\n",
    "    # Find NaN Rows\n",
    "    #na_list = data.columns[data.isna().any().tolist()]\n",
    "   # data.drop(columns=na_list, inplace=True)\n",
    "    # Handle inf values\n",
    "    data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    data.dropna(axis=1, inplace=True)\n",
    "    #data = data.drop(['volatility_kchi','volatility_kcli'], axis=1)\n",
    "\n",
    "    #df_stationary.head()\n",
    "   # print(data)\n",
    "    non_stationaries = []\n",
    "    for col in data.columns:\n",
    "        if col != \"volatility_kchi\" and col != \"volatility_kcli\":\n",
    "            dftest = adfuller(data[col].values)\n",
    "            p_value = dftest[1]\n",
    "            t_test = dftest[0] < dftest[4][\"1%\"]\n",
    "            if p_value > 0.05 or not t_test:\n",
    "                non_stationaries.append(col)\n",
    "    \n",
    "    df_stationary = data.copy()\n",
    "    \n",
    " \n",
    "  \n",
    "    loaded_model = {}\n",
    "    feature_item = {}\n",
    "    with open('../TrainedModel/indicator/{}_model_2.pkl'.format(symbol), 'rb') as f:\n",
    "        loaded_model = pickle.load(f)\n",
    "    with open('../TrainedModel/indicator/{}_features.txt'.format(symbol), 'rb') as f:\n",
    "        feature_item = pickle.load(f)\n",
    "   # print(feature_item,symbol,len(feature_item))\n",
    "    #print(\"point zero :\" ,len(df_stationary.columns))\n",
    "\n",
    "    df_stationary[non_stationaries] = df_stationary[non_stationaries].pct_change()\n",
    "    df_stationary = df_stationary.iloc[1:]\n",
    "    # Find NaN Rows\n",
    "    na_list = df_stationary.columns[df_stationary.isna().any().tolist()]\n",
    "    \n",
    "    df_stationary.drop(columns=na_list, inplace=True)\n",
    "    print(feature_item)\n",
    "    #print(\"point one :\" ,len(df_stationary.columns))\n",
    "    df_stationary = df_stationary[feature_item]\n",
    "    #print(\"point two :\" ,len(df_stationary.columns))\n",
    "\n",
    "#    print(df_stationary)\n",
    "    df_stationary = df_stationary.iloc[1:]\n",
    "    \n",
    "    df_stationary[np.isinf(df_stationary)] = np.nan  # Replace inf with NaN\n",
    "    df_stationary = df_stationary.dropna()\n",
    "    print(\"point three\",len(df_stationary.columns),symbol)\n",
    "    has_inf = \"no inf data\"\n",
    "    if np.isinf(df_stationary).any().any():\n",
    "        has_inf = \"has inf data\"\n",
    "    df_stationary.replace(np.inf, np.nan, inplace=True)\n",
    "    df_stationary.dropna(inplace=True)\n",
    "\n",
    "        \n",
    "    #print(\"check infifty value in\",has_inf)\n",
    "   # print(symbol,\" :\",len(df_stationary.columns),len(feature_item))\n",
    "    finaldata[symbol] = df_stationary \n",
    "    df_stationary = StandardScaler().fit_transform(df_stationary)\n",
    "    y_pred = loaded_model.predict(df_stationary)\n",
    "    y_pred = loaded_model.predict(df_stationary)\n",
    "\n",
    "    #print(\"got result\",symbol)\n",
    "    # Specify Target\n",
    "\n",
    "    #final training\n",
    "    loaded_models[symbol] = loaded_model\n",
    "    stock_name_only = stock_name.replace(\".NS\",\"\")\n",
    "   \n",
    "    train_yhat = loaded_model.predict(df_stationary[-1:])\n",
    "    train_yhat_proba = loaded_model.predict_proba(df_stationary[-1:])\n",
    "    greater = 0\n",
    "    one = train_yhat_proba[0][0]\n",
    "    two = train_yhat_proba[0][1]\n",
    "    if one > two:\n",
    "        greater = one*100\n",
    "    else:\n",
    "        greater = one*100\n",
    "    print(greater)\n",
    "    final_result[symbol] = [current_date,train_yhat[0],1,'indicator',stock_name_only,stock_data.head(1).Close[symbol].values[0],round(greater,2)]\n",
    "    result.append([current_date,train_yhat[0],1,'indicator',stock_name_only,stock_data.head(1).Close[symbol].values[0],round(greater,2)])\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataa = { \"data\":[]} \n",
    "\n",
    "for res in result:\n",
    "    dataa[\"data\"].append({ \n",
    "            \"cdate\": res[Icdate],\n",
    "            \"result\": str(res[Iresult]),\n",
    "            \"pre_close\" : str(res[Iclose]),            \n",
    "            \"isClassification\": res[IisClassification],\n",
    "            \"model\": res[Imodel],\n",
    "            \"stock\": res[Istock],\n",
    "            \"prob\": str(res[Iprob]),\n",
    "    })\n",
    "    \n",
    "dataa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://website-development-kerala.com/api_214124524/ai_model_daily_runner.php\"\n",
    "\n",
    "response = requests.post(url, json=dataa)\n",
    "print(response.status_code)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for symbol in symbols:\n",
    "  \n",
    "    # Set Target (for Supervised ML later on)\n",
    "    finaldata[symbol][\"TARGET\"] = -1\n",
    "    finaldata[symbol].loc[finaldata[symbol][\"Close\"].shift(-1) > finaldata[symbol][\"Close\"], \"TARGET\"] = 1\n",
    "    finaldata[symbol].dropna(inplace=True)\n",
    "    #print(finaldata)\n",
    "    df_tar = finaldata[symbol][[\"TARGET\"]]\n",
    "    df_stationar = StandardScaler().fit_transform(finaldata[symbol].iloc[:, :-1])\n",
    "   # print(df_tar)\n",
    "\n",
    "    retrained_model = loaded_models[symbol].fit(df_stationar,df_tar)\n",
    "    print(df_stationar)\n",
    "    with open('../TrainedModel/indicator/{}_model_2.pkl'.format(symbol), 'wb') as f:\n",
    "        pickle.dump(retrained_model, f)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyONf4Fw76cQlH82pdR3h9Fa",
   "mount_file_id": "182alF94XmFPfm3NMnYKs4Tu9b-3DTW7y",
   "name": "Facebook Prophet + XGBoost Challenge Template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
